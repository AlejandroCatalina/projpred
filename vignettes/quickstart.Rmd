---
title: "projpred Quick Start"
date: "`r Sys.Date()`"
output: html_vignette
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{projpred quickstart guide}
\usepackage[utf8](inputenc)
-->
```{r, child="children/SETTINGS-knitr.txt"}
```

This vignette shows how to use the main functionalities of the ```projpred```-package, which implements the projective variable selection (Goutis and Robert, 1998; Dupuis and Robert, 2003) for generalized linear models fitted with ```rstanarm```. The method is described and evaluated in comparison to many other methods in Piironen and Vehtari (2017a).


## Gaussian example
First load the packages that are needed, and specify the number of available cores (this speeds up the sampling when multiple cores are available). Uncomment the last line in the following commands (this is commented out to avoid possible problems when building the vignette along the package installation in special environments such as computing clusters). 
```{r, message=F, warning=F}
library(rstanarm)
library(projpred)
library(ggplot2)
#options(mc.cores = parallel::detectCores())
```

The package contains a simple Gaussian example dataset accessible with the `data`-command. This dataset is one of the example cases from the `glmnet`-package. The following command loads a dataframe `df_gaussian` with the predictor matrix `x` and the corresponding targets `y` into the workspace.
```{r}
data('df_gaussian', package = 'projpred')
```


We then fit the model with all the variables and sparsifying horseshoe prior (Carvalho et al., 2010) on the regression coefficients. This gives us the full Bayesian solution to the problem. To specify the prior beliefs about the number of relevant variables, we use the framework discussed by Piironen and Vehtari (2017b), in which the prior for the global shrinkage parameter is defined based on our prior guess for the number of relevant variables.
```{r, message=F, warning=F}
n <- nrow(df_gaussian$x) # 100
D <- ncol(df_gaussian$x) # 20
p0 <- 5 # prior guess for the number of relevant variables
tau0 <- p0/(D-p0) * 1/sqrt(n) # scale for tau (notice that stan_glm will automatically scale this by sigma)
prior_coeff <- hs(df=1, global_df=1, global_scale=tau0) # horseshoe prior
fit <- stan_glm(y ~ x, family=gaussian(), data=df_gaussian, prior=prior_coeff,
                # to make this vignette build fast, we use only 2 chains and
                # 800 draws. In practice, more conservative values, eg. 4 chains
                # and 2000 draws might be required for reliable inference.
                seed=1, adapt_delta=0.999, chains=2, iter=800) 

```


The variable selection can then be excecuted with the command `varsel`. This will add a field `varsel` to the `fit`-object containing information about the variable selection, such as the ordering of the variables. The search heuristic is specified by the keyword `method`. By default, `varsel` uses the LASSO type L1-penalization to find the variable ordering (Tran et al., 2012).
```{r}
fit <- varsel(fit, method='L1')
fit$varsel$chosen
```


We can then plot some statistics computed on the training data, such as the mean log predictive density (MLPD) and mean squared error (MSE) as the function of number of variables added. Setting ```deltas=F``` shows the results on absolute scale, whereas ```deltas=T``` shows the results relative to the full model.
```{r}
# plot predictive performance relative to the full model on training data 
varsel_plot(fit, statistics=c('mlpd', 'mse'), deltas=T)
```


The statistics computed on the training data typically give us a rough idea of how many variables are needed in order to capture all the predictive power of the full model. However, because these statistics are computed using the same data that was used to fit the models, the results can be biased. More reliable assessment can be obtained by cross-validating both the full model and the variable selection process. 
```{r, warning=F}
fit_cv <- cv_varsel(fit, method='L1', cv_method='LOO')
```
In this case the cross-validated results look quite similar to those computed on the training data, showing that after 6 variables the predictions do not change markedly. The model size suggested by the program is stored in the variable ```ssize``` in the field ```varsel``` (smallest model with expected loss in log predictive density less than 10 percent of the difference between the full and empty model with probability 0.9)
```{r}
fit_cv$varsel$ssize
```
```{r}
varsel_plot(fit_cv, statistics = c('mlpd', 'mse'), deltas=T)
```

We can make predictions with the submodels using method `proj_linpred`. Test inputs can be provided using the keyword `xnew`. If also the test targets `ynew` are provided, then the function evaluates the log predictive density at these points . For instance, the following computes the mean of the predictive distribution and evaluates the log density at the training points using the 6 most relevant variables.
```{r}
pred <- proj_linpred(fit, xnew=df_gaussian$x, ynew=df_gaussian$y, nv=6, integrated = TRUE)
```
Visualize the predictions
```{r}
ggplot() +
  geom_point(aes(x=pred$pred,y=df_gaussian$y)) +
  geom_abline(slope = 1, color='red') +
  labs(x = 'prediction', y = 'y')
```

## Binomial example (logistic regression)

This section shows an example of the variable selection for a logistic regression model (binary classification). Everything is very similar to the Gaussian case. First load the data (this dataset is also from the `glmnet`-package):
```{r}
data('df_binom', package = 'projpred')
```

Then fit the full model:
```{r, message=F, warning=F}
# fit the full model
n <- nrow(df_binom$x)
D <- ncol(df_binom$x)
p0 <- 5 # prior guess for the number of relevant variables
sigma <- 2 # approximate plug-in value for observation information (Piironen and Vehtari, 2017)
tau0 <- p0/(D-p0) * sigma/sqrt(n)
prior_coeff <- hs(df=1, global_df=1, global_scale=tau0)
fit <- stan_glm(y ~ x, family=binomial(), data=df_binom, prior=prior_coeff,
                seed=1, adapt_delta=0.999, chains=2, iter=800)
```

Run the variable selection
```{r}
fit <- varsel(fit, method='L1')
fit$varsel$chosen
```

Plot the MLPD and classification accuracy on the training data:
```{r}
varsel_plot(fit, statistics=c('mlpd', 'pctcorr'), deltas=F)
```

Cross-validate the full model and the variable selection:
```{r, warning=F}
fit_cv <- cv_varsel(fit, method='L1', cv_method='LOO')
```

Plot the cross-validated performance estimates on the absolute scale. In this case the cross-validated results differ from the training statistics especially for model sizes 3 to 6 varibles. About 11 variables gives predictive accurary similar to the full model (the program suggests 13 variables)
```{r}
fit_cv$varsel$ssize
```
```{r}
varsel_plot(fit_cv, statistics=c('mlpd', 'pctcorr'), deltas=F)
```

Finally, for illustration, we compute the here the predictive distribution using only the two most relevant variables, and visualize the results.
```{r}
# evaluate the predictive distribution in a 2d grid
ng <- 20
x1g <- seq(-4,4,len=ng)
x2g <- seq(-4,4,len=ng)
xnew <- cbind( rep(x1g,each=ng), rep(x2g,ng) )
vind <- fit$varsel$chosen[1:2]
pr <- proj_linpred(fit, xnew, vind=vind, transform=T, integrated=T)

# visualize the results
pp <- ggplot()
pp <- pp + geom_contour(aes(x=xnew[,1],y=xnew[,2], z=pr, colour=..level..))
pp <- pp + scale_colour_gradient(low = "red", high = "green")
pp <- pp + geom_point(aes(x=df_binom$x[,vind[1]],y=df_binom$x[,vind[2]]), color=df_binom$y+2)
pp <- pp + xlab(sprintf('Feature %d',vind[1])) + ylab(sprintf('Feature %d',vind[2]))
pp
```





### References

Carvalho, C.M., Polson, N.G., Scott, J.G. (2010). The horseshoe estimator for sparse signals. _Biometrika_ 97(2):465–480. doi:10.1093/biomet/asq017

Dupuis, J. A. and Robert, C. P. (2003). Variable selection in qualitative models via an entropic explanatory power. _Journal of Statistical Planning and Inference_, 111(1-2):77–94.

Goutis, C. and Robert, C. P. (1998). _Model choice in generalised linear models: a Bayesian approach via Kullback–Leibler projections_. Biometrika, 85(1):29–37.

Piironen, Juho and Vehtari, Aki (2017a). Comparison of Bayesian predictive methods for model selection. _Statistics and Computing_ 27(3):711-735. DOI 10.1007/s11222-016-9649-y. [Online](http://link.springer.com/article/10.1007/s11222-016-9649-y)

Piironen, Juho and Vehtari, Aki (2017b). On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior. In _Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS 2017)_, accepted for publication. [Preprint](https://arxiv.org/abs/1610.05559)

Tran, M.N., Nott, D.J., Leng, C. (2012): The predictive Lasso. _Statistics and Computing_ 22(5):1069-1084. doi:10.1007/s11222-011-9279-3
